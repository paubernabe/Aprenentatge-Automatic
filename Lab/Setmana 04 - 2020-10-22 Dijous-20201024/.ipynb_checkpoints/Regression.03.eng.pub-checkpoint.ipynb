{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression 03 \n",
    "\n",
    "# A. Shrinkage (regularization) methods\n",
    "\n",
    "bias + variança\n",
    "# B. Orthogonalization methods\n",
    "\n",
    "agafar els predictors i reemplaçar-los per una coleccio de comb lineals, per a que els nous predictors siguin ortogonals entre ells (combinacions lineals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Josep Fortiana 2020-10-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both families of methods are applicable when there are many predictors, possibly multicollinear. \n",
    "\n",
    "Shrinkage, or regularization, methods replace the ordinary least squares condition with penalized least squares, where the penalty term purpose is to diminish the regression coefficients variance (dispersion, unstability). This is the shrinkage in the name.\n",
    "\n",
    "Orthogonalization methods replace the set of observed predictor variables with a new set of orthogonal variables, derived as linear combinations of the old ones in such a way that the _prediction space,_ that is, the space of columns of $X$, the regression matrix is conserved.\n",
    "\n",
    "In this laboratory we see two shrinkage methods: Ridge regression and the Lasso, and two orthogonalization methods, Principal Components Regression (PCR) and Partial Least Squares (PLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1. Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Longley dataset and the `lm.ridge` function in the `MASS`  package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longley's Economic Regression Data\n",
    "\n",
    "### Description\n",
    "\n",
    "A macroeconomic data set which provides a well-known example for a highly collinear regression.\n",
    "\n",
    "### Format\n",
    "\n",
    "A data frame with 7 economical variables, observed yearly from 1947 to 1962 (n=16).\n",
    "\n",
    "01. `GNP.deflator`: GNP implicit price deflator (1954=100)\n",
    "02. `GNP`:          Gross National Product.\n",
    "03. `Unemployed`:   number of unemployed.\n",
    "04. `Armed.Forces`: number of people in the armed forces.\n",
    "05. `Population`:   noninstitutionalized population ≥ 14 years of age.\n",
    "06. `Year`:         the year (time).\n",
    "07. `Employed`:     number of people employed.\n",
    "\n",
    "The regression `lm(Employed ~ .)` is known to be highly collinear.\n",
    "\n",
    "### Source\n",
    "\n",
    "J. W. Longley (1967) _An appraisal of least-squares programs from the point of view of the user._\n",
    "Journal of the American Statistical Association 62, 819–841.\n",
    "\n",
    "### References\n",
    "\n",
    "Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) _The New S Language._ Wadsworth & Brooks/Cole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: MASS\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t16 obs. of  7 variables:\n",
      " $ GNP.deflator: num  83 88.5 88.2 89.5 96.2 ...\n",
      " $ GNP         : num  234 259 258 285 329 ...\n",
      " $ Unemployed  : num  236 232 368 335 210 ...\n",
      " $ Armed.Forces: num  159 146 162 165 310 ...\n",
      " $ Population  : num  108 109 110 111 112 ...\n",
      " $ Year        : int  1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 ...\n",
      " $ Employed    : num  60.3 61.1 60.2 61.2 63.2 ...\n"
     ]
    }
   ],
   "source": [
    "require(MASS)\n",
    "data(longley)\n",
    "str(longley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 9\n",
      " $ coef  : Named num [1:6] 0.157 -3.447 -1.828 -0.696 -0.344 ...\n",
      "  ..- attr(*, \"names\")= chr [1:6] \"GNP.deflator\" \"GNP\" \"Unemployed\" \"Armed.Forces\" ...\n",
      " $ scales: Named num [1:6] 10.45 96.24 90.48 67.38 6.74 ...\n",
      "  ..- attr(*, \"names\")= chr [1:6] \"GNP.deflator\" \"GNP\" \"Unemployed\" \"Armed.Forces\" ...\n",
      " $ Inter : int 1\n",
      " $ lambda: num 0\n",
      " $ ym    : num 65.3\n",
      " $ xm    : Named num [1:6] 102 388 319 261 117 ...\n",
      "  ..- attr(*, \"names\")= chr [1:6] \"GNP.deflator\" \"GNP\" \"Unemployed\" \"Armed.Forces\" ...\n",
      " $ GCV   : Named num 0.00836\n",
      "  ..- attr(*, \"names\")= chr \"0\"\n",
      " $ kHKB  : num 0.00428\n",
      " $ kLW   : num 0.0323\n",
      " - attr(*, \"class\")= chr \"ridgelm\"\n"
     ]
    }
   ],
   "source": [
    "longley.ridge.1<-lm.ridge(Employed ~ .,data=longley)\n",
    "str(longley.ridge.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients(longley.ridge.1) #coefs de la regressió"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dl-inline {width: auto; margin:0; padding: 0}\n",
       ".dl-inline>dt, .dl-inline>dd {float: none; width: auto; display: inline-block}\n",
       ".dl-inline>dt::after {content: \":\\0020\"; padding-right: .5ex}\n",
       ".dl-inline>dt:not(:first-of-type) {padding-left: .5ex}\n",
       "</style><dl class=dl-inline><dt>GNP.deflator</dt><dd>10.4488766591199</dd><dt>GNP</dt><dd>96.2387346946181</dd><dt>Unemployed</dt><dd>90.4791116691444</dd><dt>Armed.Forces</dt><dd>67.3821259566474</dd><dt>Population</dt><dd>6.7352163755146</dd><dt>Year</dt><dd>4.60977222864644</dd></dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[GNP.deflator] 10.4488766591199\n",
       "\\item[GNP] 96.2387346946181\n",
       "\\item[Unemployed] 90.4791116691444\n",
       "\\item[Armed.Forces] 67.3821259566474\n",
       "\\item[Population] 6.7352163755146\n",
       "\\item[Year] 4.60977222864644\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "GNP.deflator\n",
       ":   10.4488766591199GNP\n",
       ":   96.2387346946181Unemployed\n",
       ":   90.4791116691444Armed.Forces\n",
       ":   67.3821259566474Population\n",
       ":   6.7352163755146Year\n",
       ":   4.60977222864644\n",
       "\n"
      ],
      "text/plain": [
       "GNP.deflator          GNP   Unemployed Armed.Forces   Population         Year \n",
       "   10.448877    96.238735    90.479112    67.382126     6.735216     4.609772 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "longley.ridge.1$scales #lm.ridge, les variables predictores les escala de manera que tinguin mean = 0 variança =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               GNP.deflator           GNP    Unemployed  Armed.Forces \n",
      "-3.482259e+03  1.506187e-02 -3.581918e-02 -2.020230e-02 -1.033227e-02 \n",
      "   Population          Year \n",
      "-5.110411e-02  1.829151e+00 \n"
     ]
    }
   ],
   "source": [
    "print(longley.ridge.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Length Class  Mode   \n",
       "coef   6      -none- numeric\n",
       "scales 6      -none- numeric\n",
       "Inter  1      -none- numeric\n",
       "lambda 1      -none- numeric\n",
       "ym     1      -none- numeric\n",
       "xm     6      -none- numeric\n",
       "GCV    1      -none- numeric\n",
       "kHKB   1      -none- numeric\n",
       "kLW    1      -none- numeric"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(longley.ridge.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical methods to find an optimal regularization parameter\n",
    "\n",
    "`kHKB` is an estimate of the optimal $\\lambda$, proposed by Hoerl, Kennard and Baldwin (1975). \n",
    "\n",
    "`kLW` is another estimate, proposed by Lawless, Wang (1976). \n",
    "\n",
    "`GCV` is the Generalized Cross-Validation statistic evaluated for each of the $\\lambda$ values being tested.\n",
    "\n",
    "\n",
    "\n",
    "La regressió ridge: recordeu que quan tenieu una reg amb resposta Y i matriu de regressió X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longley.ridge<-lm.ridge(Employed ~ .,data=longley,lambda=seq(0,0.1,by=0.001))\n",
    "options(repr.plot.width=5, repr.plot.height=5)\n",
    "plot(longley.ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select(longley.ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `broom` package has functions to gather and visualize the output of `lm.ridge` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"broom\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(broom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy(longley.ridge) \n",
    "# long output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glance(longley.ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Acetylene dataset and the  `genridge`  package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `genridge` package includes the `Acetylene` dataset, with new variable names. We recover the linear model we tried above on these data and then we try a second linear model with quadratic terms. As a matter of fact this dataset originates from the paper: Marquardt, Donald W. and Snee, Ronald D. (1975), _\"Ridge Regression in Practice\",_ The American Statistician, Vol. 29, No. 1, pp. 3-20. In this paper the authors start with the model with all six quadratic terms:\n",
    "\n",
    "$$\n",
    " \\text{temp}^2,\\mskip10mu \\text{ratio}^2,\\mskip10mu \\text{time}^2,\\mskip10mu \\text{temp}\\cdot\\text{ratio},\\mskip10mu \\text{temp}\\cdot\\text{time},\\mskip10mu\\text{ratio}\\cdot \\text{time}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"genridge\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "#install.packages(\"car\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(genridge)\n",
    "require(car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acetylene Data\n",
    "\n",
    "### Description\n",
    "\n",
    "The data consist of measures of yield of a chemical manufacturing process for acetylene in relation to numeric parameters.\n",
    "\n",
    "Marquardt and Snee (1975) used these data to illustrate ridge regression in a model containing quadratic and interaction terms, \n",
    "particularly the need to center and standardize variables appearing in high-order terms.\n",
    "\n",
    "### Usage\n",
    "\n",
    "`data(Acetylene)`\n",
    "\n",
    "### Format\n",
    "\n",
    "A data frame with 16 observations on the following 4 variables.\n",
    "\n",
    "01. `yield`: conversion percentage yield of acetylene\n",
    "02. `temp`:  reactor temperature (celsius)\n",
    "03. `ratio`: H2 to N-heptone ratio\n",
    "04. `time`:  contact time (sec)\n",
    "\n",
    "### Details\n",
    "\n",
    "Typical models for these data include the interaction of temp:ratio, and a squared term in temp\n",
    "\n",
    "### Source\n",
    "\n",
    "SAS documentation example for PROC REG, Ridge Regression for Acetylene Data.\n",
    "\n",
    "### References\n",
    "\n",
    "Marquardt, D.W., and Snee, R.D. (1975), _\"Ridge Regression in Practice\",_ The American Statistician, 29, 3-20.\n",
    "\n",
    "Marquardt, D.W. (1980), _\"A Critique of Some Ridge Regression Methods: Comment\",_ \n",
    "Journal of the American Statistical Association, Vol. 75, No. 369 (Mar., 1980), pp. 87-91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(Acetylene)\n",
    "str(Acetylene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with only linear terms (main effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acetylene.lm1<-lm(yield~temp+ratio+time,data=Acetylene)\n",
    "summary(Acetylene.lm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(Acetylene.lm1)\n",
    "X.Acetylene.lm1<-model.matrix(Acetylene.lm1)\n",
    "kappa(X.Acetylene.lm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model from the original paper by Marquardt and Snee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acetylene.lm2 <- lm(yield ~ temp + ratio + time + I(temp^2)+ I(ratio^2)+ I(time^2) \n",
    "                    + temp:ratio+temp:time+ratio:time, data=Acetylene)\n",
    "summary(Acetylene.lm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(Acetylene.lm2)\n",
    "X.Acetylene.lm2<-model.matrix(Acetylene.lm2)\n",
    "kappa(X.Acetylene.lm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A third model, with fewer quadratic terms, used by Michael Friendly to illustrate genridge\n",
    "Acetylene.lm3 <- lm(yield ~ temp + ratio + time + I(time^2) + temp:time, data=Acetylene)\n",
    "summary(Acetylene.lm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(Acetylene.lm3)\n",
    "X.Acetylene.lm3<-model.matrix(Acetylene.lm3)\n",
    "kappa(X.Acetylene.lm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression with the `ridge` function from `genridge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y<- Acetylene[,\"yield\"]\n",
    "X0<-X.Acetylene.lm3[,-1]\n",
    "lambda <- c(0, 0.0005, 0.001, 0.002, 0.005, 0.01)\n",
    "Acetylene.ridge.1 <- ridge(y, X0, lambda=lambda)\n",
    "summary(Acetylene.ridge.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphic method to find a good regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot(Acetylene.ridge.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot(Acetylene.ridge.1, X=\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs(Acetylene.ridge.1, radius=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Fearn dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset from the paper by Fearn, T. (1983), _A Misuse of Ridge Regression in the Calibration of a Near Infrared Reflectance Instrument,_ Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 32, No. 1(1983), pp. 73-79. This paper, with intended controversial title and contents, found its rebuttal in the paper by Hoerl, Arthur E., Kennard, Robert W.  and Hoerl, Roger W. (1985), _Practical Use of Ridge Regression: A Challenge Met,_ Journal of the Royal Statistical Society. Series C (Applied Statistics), Vol. 34, No. 2(1985), pp. 114-120."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are the results of an experiment performed to calibrate a near infrared (NIR) reflectance instrument for the measurement of protein content in ground wheat samples. \n",
    "\n",
    "The protein content measurements `y` were made by the standard Kjeldahl method, and the six values `x1` - `x6` are measurements of the reflectance of NIR radiation by the wheat samples at six different wavelengths in the range 1680-2310 nm. These measurements are made on a $\\log (1/R)$ scale, where $R$ is the reflectance, and are commonly referred to as _\"log values\"._ \n",
    "\n",
    "The aim of the calibration is to find a linear combination of the log values which predicts protein content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fearn.1<-read.table(\"Fearn.data.1.txt\", header=TRUE)\n",
    "Fearn.2<-read.table(\"Fearn.data.2.txt\", header=TRUE)\n",
    "str(Fearn.1)\n",
    "str(Fearn.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the regression `y~x1+x2+x3+x4+x5+x6` with the Fearn dataset and:\n",
    "\n",
    "1. Ordinary Least Squares (OLS), selecting the best predictors subset\n",
    "\n",
    "2. Ridge regression\n",
    "\n",
    "Compare prediction errors. Which one is better?\n",
    "\n",
    "3. After working through the following section on the lasso, repeat with this method.\n",
    "\n",
    "NOTE: the data frames `Fearn.1` and `Fearn.2` were used as train and test subsets in the original paper. You may choose to follow this selection or merge both subsets and partition the joint dataset in some other way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The `Hitters` dataset in the `ISLR` package \n",
    "\n",
    "### Ridge regression following ISLR - Chap 6 - Laboratory 2 - Using the `glmnet` package\n",
    "\n",
    "[Code from the ISLR website](http://www-bcf.usc.edu/~gareth/ISL/Chapter%206%20Labs.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseball Data\n",
    "\n",
    "### Description\n",
    "\n",
    "Major League Baseball Data from the 1986 and 1987 seasons.\n",
    "\n",
    "### Usage\n",
    "\n",
    "`data(Hitters)`\n",
    "\n",
    "### Format\n",
    "\n",
    "A data frame with 322 observations of major league players on the following 20 variables.\n",
    "\n",
    "01. `AtBat`:      Number of times at bat in 1986\n",
    "02. `Hits`:       Number of hits in 1986\n",
    "03. `HmRun`:      Number of home runs in 1986\n",
    "04. `Runs`        Number of runs in 1986\n",
    "05. `RBI`:        Number of runs batted in in 1986\n",
    "06. `Walks`:      Number of walks in 1986\n",
    "07. `Years`:      Number of years in the major leagues\n",
    "08. `CAtBat`:     Number of times at bat during his career\n",
    "09. `CHits`:      Number of hits during his career\n",
    "10. `CHmRun`:     Number of home runs during his career\n",
    "11. `CRuns`:      Number of runs during his career\n",
    "12. `CRBI`:       Number of runs batted in during his career\n",
    "13. `CWalks`:     Number of walks during his career\n",
    "14. `League`:     A factor with levels A and N indicating player's league at the end of 1986\n",
    "15. `Division`:   A factor with levels E and W indicating player's division at the end of 1986\n",
    "16. `PutOuts`:    Number of put outs in 1986\n",
    "17. `Assists`:    Number of assists in 1986\n",
    "18. `Errors`:     Number of errors in 1986\n",
    "19. `Salary`:     1987 annual salary on opening day in thousands of dollars\n",
    "20. `NewLeague`:  A factor with levels A and N indicating player's league at the beginning of 1987\n",
    "\n",
    "### Source\n",
    "\n",
    "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. \n",
    "This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. \n",
    "The salary data were originally from Sports Illustrated, April 20, 1987. \n",
    "The 1986 and career statistics were obtained from _The 1987 Baseball Encyclopedia Update_ \n",
    "published by Collier Books, Macmillan Publishing Company, New York.\n",
    "\n",
    "### References\n",
    "\n",
    "James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) _An Introduction to Statistical Learning with applications in R,_\n",
    "www.StatLearning.com, Springer-Verlag, New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"ISLR\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(ISLR)\n",
    "#fix(Hitters)\n",
    "names(Hitters)\n",
    "dim(Hitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(is.na(Hitters$Salary))\n",
    "Hitters=na.omit(Hitters)\n",
    "dim(Hitters)\n",
    "sum(is.na(Hitters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare x, y for the glmnet syntax\n",
    "x<-model.matrix(Salary~.,Hitters)[,-1]\n",
    "y<-Hitters$Salary                        # Response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"glmnet\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax of the `glmnet()`function\n",
    "\n",
    "`alpha=0` is for _ridge regression_\n",
    "\n",
    "`alpha=1` is for _lasso regression_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When lambda goes to infinity penalization on coefficients beta01 through beta19 is so high \n",
    "that it pushes all of them down to zero, resulting in a model with no predictors, only the intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  A grid of lambda values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid<-10^seq(10,-2,length=100)\n",
    "ridge.mod<-glmnet(x,y,alpha=0,lambda=grid)\n",
    "str(ridge.mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(coef(ridge.mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the beta regression coefficients with a large lambda (small absolute values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(ridge.mod$lambda[50],2)\n",
    "round(coef(ridge.mod)[,50],2)\n",
    "round(sqrt(sum(coef(ridge.mod)[-1,50]^2)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with a smaller lambda (larger absolute values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(ridge.mod$lambda[60],2)\n",
    "round(coef(ridge.mod)[,60],2)\n",
    "round(sqrt(sum(coef(ridge.mod)[-1,60]^2)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract now the regression coefficients with the `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(predict(ridge.mod,s=50,type=\"coefficients\")[1:20,],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split randomly the dataset into 'train' and 'test' subsets\n",
    "set.seed(1)\n",
    "train<-sample(1:nrow(x), nrow(x)/2)\n",
    "test<-(-train)\n",
    "y.test<-y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust model with the 'train' subset\n",
    "ridge.mod<-glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)\n",
    "#\n",
    "# Then we evaluate prediction error (sum of squares) on the 'test' subset for three lambda values\n",
    "# (lambda=4, lambda=1.0e10, lambda=0)\n",
    "#\n",
    "ridge.pred<-predict(ridge.mod,s=4,newx=x[test,])\n",
    "round(mean((ridge.pred-y.test)^2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model with no predictors (other than the intercept) has always a predicted value equal to the mean of y\n",
    "# With a large lambda, the model tends to the no predictor one\n",
    "round(mean((mean(y[train])-y.test)^2),2)\n",
    "ridge.pred<-predict(ridge.mod,s=1e10,newx=x[test,])\n",
    "round(mean((ridge.pred-y.test)^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With lambda equal to zero, the ridge regression model reduces to ordinary least squares\n",
    "#\n",
    "## Warning\n",
    "#\n",
    "# predict.glmnet with 'exact' computation requires re-entering the original training dataset\n",
    "#\n",
    "ridge.pred<-predict(ridge.mod,x=x[train,],y=y[train],s=0,newx=x[test,],exact=TRUE)\n",
    "round(mean((ridge.pred-y.test)^2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same, with no 'exact' computation\n",
    "#\n",
    "ridge.pred<-predict(ridge.mod,s=0,newx=x[test,])\n",
    "round(mean((ridge.pred-y.test)^2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare an ordinary least squares regression with ridge regression with lambda=0 \n",
    "ols<-lm(Salary~.,data=Hitters, subset=train)\n",
    "summary(ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols.yhat<-predict.lm(ols,newdata=Hitters[test,],type=\"response\")\n",
    "str(ols.yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols.residuals<-ols.yhat-y.test\n",
    "round(mean(ols.residuals^2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.yhat<-predict(ridge.mod,x=x[train,],y=y[train],s=0,newx=x[test,],exact=TRUE)\n",
    "#ridge.yhat<-predict(ridge.mod,s=0,newx=x[test,],type=\"response\")\n",
    "ridge.residuals<-ridge.yhat-y.test\n",
    "round(mean(ridge.residuals^2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a k-fold cross-validation feature in the glmnet package which we can take advantege of\n",
    "# \n",
    "# By default k=10\n",
    "set.seed(1)\n",
    "cv.out<-cv.glmnet(x[train,],y[train],alpha=0)\n",
    "plot(cv.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestlam<-cv.out$lambda.min\n",
    "round(bestlam,3)\n",
    "round(log(bestlam),3)\n",
    "plot(cv.out)\n",
    "abline(v=log(bestlam),lwd=3,col=\"cyan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean quadratic error with the optimal lambda and the full dataset\n",
    "# \n",
    "# Coefficients of this model\n",
    "#\n",
    "# We observe that none of these coefficients is zero, hence there is no variable selectioin in ridge regression. \n",
    "# To be compared with the lasso below.\n",
    "ridge.pred<-predict(ridge.mod,s=bestlam,newx=x[test,])\n",
    "round(mean((ridge.pred-y.test)^2),3)\n",
    "out<-glmnet(x,y,alpha=0)\n",
    "round(predict(out,type=\"coefficients\",s=bestlam)[1:20,],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Regression with the _Lasso_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same `Hitters` dataset as above and `glmnet` with `alpha=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.mod<-glmnet(x[train,],y[train],alpha=1,lambda=grid)\n",
    "plot(lasso.mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "cv.out<-cv.glmnet(x[train,],y[train],alpha=1)\n",
    "bestlam<-cv.out$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=6, repr.plot.height=6)\n",
    "plot(cv.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(bestlam,3)\n",
    "round(log(bestlam),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cv.out)\n",
    "abline(v=log(bestlam),lwd=3,col=\"cyan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic error on the test subset with the optimal lambda \n",
    "lasso.pred<-predict(lasso.mod,s=bestlam,newx=x[test,])\n",
    "round(mean((lasso.pred-y.test)^2),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection feature of the _Lasso_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic error on the full dataset with the optimal lambda\n",
    "# Coefficients in this model:\n",
    "#\n",
    "# Now we see there are zero coefficients: this is equivalent to discarding these variables.\n",
    "#\n",
    "# Compare with the ridge regression above\n",
    "#\n",
    "out<-glmnet(x,y,alpha=1,lambda=grid)\n",
    "lasso.coef<-predict(out,type=\"coefficients\",s=bestlam)[1:20,]\n",
    "round(lasso.coef,3)\n",
    "round(lasso.coef[lasso.coef!=0],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Orthogonalization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following  ISLR - Cap 6 - Laboratory 3 - PCR and PLS \n",
    "\n",
    "[Code from the ISLR website](http://www-bcf.usc.edu/~gareth/ISL/Chapter%206%20Labs.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"pls\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(pls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B1. Principal Components Regression (PCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Components Regression\n",
    "set.seed(2)\n",
    "pcr.fit<-pcr(Salary~., data=Hitters,scale=TRUE,validation=\"CV\")\n",
    "summary(pcr.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationplot(pcr.fit,val.type=\"MSEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with hold-out\n",
    "#\n",
    "# Training the model, selecting number of principal components included in the model\n",
    "set.seed(1)\n",
    "pcr.fit<-pcr(Salary~., data=Hitters,subset=train,scale=TRUE, validation=\"CV\")\n",
    "validationplot(pcr.fit,val.type=\"MSEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The minimum of the graph (optimal number of orthogonal variables) appears at 5 variables (principal components).\n",
    "# Fit the model for this number\n",
    "pcr.pred<-predict(pcr.fit,x[test,],ncomp=5)\n",
    "round(mean((pcr.pred-y.test)^2),3)\n",
    "pcr.fit<-pcr(y~x,scale=TRUE,ncomp=5)\n",
    "summary(pcr.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2. Partial Least Squares (PLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Least Squares\n",
    "set.seed(1)\n",
    "pls.fit<-plsr(Salary~., data=Hitters,subset=train,scale=TRUE, validation=\"CV\")\n",
    "summary(pls.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationplot(pls.fit,val.type=\"MSEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The minimum of the graph (optimal number of orthogonal variables) appears at 2 variables.\n",
    "# Fit the model for this number\n",
    "pls.pred<-predict(pls.fit,x[test,],ncomp=2)\n",
    "round(mean((pls.pred-y.test)^2),3)\n",
    "pls.fit<-plsr(Salary~., data=Hitters,scale=TRUE,ncomp=2)\n",
    "summary(pls.fit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
