{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Josep Fortiana 2020-10-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multicollinearity and condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acetylene data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the files: `Acetylene.dat.txt` (data description), `Acetylene.txt` (actual data). We will use these data for multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t16 obs. of  4 variables:\n",
      " $ X1: int  1300 1300 1300 1300 1300 1300 1200 1200 1200 1200 ...\n",
      " $ X2: num  7.5 9 11 13.5 17 23 5.3 7.5 11 13.5 ...\n",
      " $ X3: num  0.012 0.012 0.0115 0.013 0.0135 0.012 0.04 0.038 0.032 0.026 ...\n",
      " $ Y : num  49 50.2 50.5 48.5 47.5 44.5 28 31.5 34.5 35 ...\n"
     ]
    }
   ],
   "source": [
    "Acetylene<-read.table(\"Acetylene.txt\",header=TRUE)\n",
    "str(Acetylene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A description of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Acetylene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(cor(Acetylene),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model with a linear influence of each of the  three available predictors _(main effects)_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acetylene.lm.1<-lm(Y~X1+X2+X3,data=Acetylene)\n",
    "summary(Acetylene.lm.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax for prediction\n",
    "\n",
    "Assume we have a (or several) new observation(s) of $X1$, $X2$, $X3$. What does the model `Acetylene.lm.1` predict for the corresponding value(s) of $Y$?\n",
    "\n",
    "$X1=1200$, $X2=11.0$, $X3=0.033$ \n",
    "\n",
    "Here I have chosen the median values, just as an example. Any value within the predictor ranges will do. Prediction for values outside this range (extrapolation) is possible but one should be wary of large errors. \n",
    "\n",
    "Procedure is to build a `data.frame` with the new values (including variable names) and use the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.observation<-data.frame(X1=1200,X2=11.0,X3=0.033)\n",
    "Acetylene.prediction.1<-predict(Acetylene.lm.1,newdata=new.observation,type=\"response\")\n",
    "Acetylene.prediction.2<-predict(Acetylene.lm.1,newdata=new.observation,type=\"response\", interval=\"confidence\")\n",
    "Acetylene.prediction.3<-predict(Acetylene.lm.1,newdata=new.observation,type=\"response\", interval=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(Acetylene.prediction.1)\n",
    "Acetylene.prediction.1\n",
    "#str(Acetylene.prediction.2)\n",
    "Acetylene.prediction.2\n",
    "#str(Acetylene.prediction.3)\n",
    "Acetylene.prediction.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the _regression (or model) matrix,_ i.e., the result of pasting a column of ones to the matrix containing the values of  `X1`, `X2`, `X3`. The new column will correspond to the `(Intercept)` regression term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X<-model.matrix(Acetylene.lm.1)\n",
    "X\n",
    "round(cor(X[,-1]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix $Q=X'\\cdot X$ of cross-products, its determinant and its inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q<- t(X) %*% X\n",
    "round(Q,3)\n",
    "round(det(Q),3)\n",
    "Q1<-solve(Q)\n",
    "round(Q1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression coefficients estimates $\\hat{\\beta}$:\n",
    "\n",
    "$$\n",
    "    \\hat{\\beta}=(X'\\cdot X)^{-1}\\cdot X' \\cdot y.\n",
    "$$\n",
    "\n",
    "$y$ is the column of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y<-as.matrix(Acetylene$Y)\n",
    "Q1%*%t(X) %*% y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $V$, variances and covariances matrix of the regression coefficients estimators $\\hat{\\beta}$, in two ways:\n",
    "\n",
    "(1) Directly from the above formula, using the fact that $\\operatorname{Var}(y)=\\sigma^{2}\\,I$, substituting the ML estimate $\\hat{\\sigma}^{2}$.\n",
    "\n",
    "(2) With the `vcov()` function in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.hat<-summary(Acetylene.lm.1)$sigma\n",
    "V<-sigma.hat^2*Q1\n",
    "round(V,3)\n",
    "vcov(Acetylene.lm.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal entries in $V$ are the variances of the coefficient estimators. We observe two of them are quite large, indicating model instability. \n",
    "\n",
    "This behaviour can be more precisely detected by using the _Variance Inflation Factors (VIF),_ which we compute with the `vif()` function from the `car` package. Intuitively, VIF's quantify _multicollinearity_ in the data, that is, linear dependences in the set of predictors. The VIF of a predictor is the proportion of its actual variance relative to the one it would have if it were linearly independent from the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"car\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(car)\n",
    "#?vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(Acetylene.lm.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commonly accepted rule of thumb is that a VIF larger than 10 is too large, indicating multicollinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quantity used in multicollinearity detection is the _condition number_ of the model matrix $X$. In general, the condition number $\\kappa(A)$ of a matrix $A$ measures the numerical inaccuracy introduced by solving the equation $A\\cdot x = b$ for a given column vector $b$. Technically it is the ratio between the maximum and minimum singular values of $A$. When $\\kappa\\approx1$ there is no significant precision loss. When $\\kappa=10^k$ the precision loss of $k$ significant decimal digits:\n",
    "\n",
    "The `kappa()` function in R gives the condition number of a matrix. We check the condition number of the model matrix $X$ of this regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these data have bad condition for regression. The practical consequence of this fact is that predictions $\\hat{y}$ obtained from this model are unreliable. \n",
    "\n",
    "The predicted $\\hat{y}$ for a new $x$ can be computed as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "newy<-predict(Acetylene.lm.1,newdata=data.frame(X1=1100,X2=8,X3=0.02), type=\"response\")\n",
    "round(newy,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, due to the bad condition of the model it would be unadvisable to rely upon this value, which possibly will have a large error (and we do not know how large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(predict(Acetylene.lm.1,newdata=data.frame(X1=1100,X2=8,X3=0.02), type=\"response\",interval=\"confidence\"),3)\n",
    "round(predict(Acetylene.lm.1,newdata=data.frame(X1=1100,X2=8,X3=0.02), type=\"response\",interval=\"prediction\"),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should restate the model, either replacing the original predictors with suitable linear combinations, designed to diminish multicollinearity, such as _PCR_ and _PLS,_ or by means of _regularization._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPA (Grade Point Average) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPA dataset has 30 observacions (for 30 students) of 5 variables:\n",
    "\n",
    "`gpa`:   \"graduate grade point average\"\n",
    "\n",
    "`greq`:  \"GRE exam quantitative score\"\n",
    "\n",
    "`grev`:  \"GRE exam verbal score\"\n",
    "\n",
    "`mat`:   \"Miller Analogies Test score\"\n",
    "\n",
    "`ar`:    \"rating by professors\"\n",
    "\n",
    "#### Read the data file into R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa<-read.table(\"gpa.txt\",header=TRUE)\n",
    "str(gpa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a linear model to predict the response `gpa` from the remaining four variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa.lm.1<-lm(gpa~.,data=gpa)\n",
    "summary(gpa.lm.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute VIF and condition number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(gpa.lm.1)\n",
    "X.gpa<-model.matrix(gpa.lm.1)\n",
    "kappa(X.gpa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number $\\kappa$ is not small, but the VIF's are acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the effect of adding a new, redundant, variable\n",
    "\n",
    "A _\"new\"_ artificial predictor variable adding no new information. A linear combination plus a random noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new<-gpa$greq+gpa$grev+gpa$mat+rnorm(30,0,0.5)\n",
    "gpanew<-data.frame(gpa,new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New linear model using the new, enlarged, predictor set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa.lm.2<-lm(gpa~.,data=gpanew)\n",
    "summary(gpa.lm.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIF's and condition number of the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif(gpa.lm.2)\n",
    "X.gpa.2<-model.matrix(gpa.lm.2)\n",
    "kappa(X.gpa.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longley dataset\n",
    "\n",
    "This is a classic infamous dataset, designed to reveal computational weaknesses in statistical software (of its time, fifty years ago). The source is:  Longley, James W. (1967), _\"An Appraisal of Least Squares Programs for the Electronic Computer from the Point of View of the User\",_ Journal of the American Statistical Association, Vol. 62, No. 319, pp. 819-841.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(longley)\n",
    "str(longley)\n",
    "# ?longley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Prepare a linear model to predict `Employed` from the remaining variables.\n",
    "\n",
    "Obtain VIF's and condition number. Repeat after standardizing the predictors. What happens with VIF and `kappa`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Insert here your code\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Polynomial regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The  `cars` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take another classic dataset: `cars`, stopping distance of several vehicles as a function of the initial speed. These data were registered in the 1920's. From Physics, stopping distance should be a quadratic function of speed. Firstly we fit a simple linear regression and then we try a linear regression with a quadratic term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(cars)\n",
    "# ?cars\n",
    "str(cars)\n",
    "plot(dist~speed,data=cars,pch=19,col=\"red\",cex=0.8,main=\"Stopping distance vs. speed\")\n",
    "cars.lm.1<-lm(dist~speed,data=cars)\n",
    "abline(cars.lm.1,lwd=2.1,col=\"blue\")\n",
    "summary(cars.lm.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression with a quadratic term\n",
    "\n",
    "Note the model syntax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.lm.2<-lm(dist~speed+I(speed^2),data=cars)\n",
    "summary(cars.lm.2)\n",
    "plot(dist~speed,data=cars,pch=19,col=\"red\",cex=0.8,main=\"Stopping distance vs. speed\")\n",
    "s<-seq(min(cars$speed),max(cars$speed),length=400)\n",
    "d<-predict(cars.lm.2,newdata=data.frame(speed=s))\n",
    "lines(s,d,lwd=2.1,col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear model with the logarithms of both predictor and response\n",
    "\n",
    "This is an expedient device when one expects a power function relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.lm.3<- lm(log(dist) ~ log(speed), data = cars)\n",
    "summary(cars.lm.3)\n",
    "plot(log(dist)~log(speed),data=cars,pch=19,col=\"red\",cex=0.8,main=\"Log(Stopping distance) vs. Log(Speed)\")\n",
    "abline(cars.lm.3,lwd=2.1,col=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparing regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `hills` dataset\n",
    "\n",
    "#### Package DAAG\n",
    "\n",
    "From the book: Maindonald, J., Braun, J. (2003), _Data Analysis and Graphics Using R._ Cambridge University Press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"DAAG\",dependencies=TRUE,repos=\"https://cloud.r-project.org\")\n",
    "require(DAAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `hills` dataset and read its help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(hills)\n",
    "str(hills)\n",
    "#?hills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition about the physics underlying this problem suggests a power function relationship which, in turn, motivates us to apply a logarithmic transformation. \n",
    "\n",
    "We prepare a new `data.frame` with the logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loghills<-log(hills)\n",
    "names(loghills)<-c(\"logdist\",\"logclimb\",\"logtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the transformation improve the linear regression?\n",
    "Compare both settings, scatterplots, correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Insert here your code\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing regressions: relevant quantities\n",
    "\n",
    "To this end we have the following available quantities:  \n",
    "\n",
    "1. $\\operatorname{ResSS}$, the Residual Sum of Squares\n",
    "2. The coefficient of determination $R^2$\n",
    "3. The $F$ statistic.\n",
    "4. The $\\operatorname{AIC}$ statistic (Akaike Information Criterion). \n",
    "\n",
    "$\\operatorname{ResSS}$ is a goodness-of-fit measure: smaller values indicate a better fit. However, when a model $L_1$ is obtained by adding predictors to an initial one $L_0$, inevitably $\\operatorname{ResSS}$ decreases ($\\operatorname{ResSS}_1\\leq \\operatorname{ResSS}_0$) by geometry, since in the $L_1$ we are projecting on a subspace larger than in $L_0$. Similarly $R_1^2\\geq R_0^2$.\n",
    "\n",
    "From a statistical perspective we need to decide whether an observed decrease in $\\operatorname{ResSS}$, or\n",
    "increase in $R^2$ is significant. This is the purpose of $F$ and $\\operatorname{AIC}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get a new model $L_1$ by adding a single predictor to a model $L_0$, the comparison $F$ statistic is defined as the relative decrease in $\\operatorname{ResSS}$, that is, the ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F01<-(ResSS0-ResSS1)/(ResSS1/df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ResSS0` and `ResSS` are, respectively, the residual sum of squares of $L_0$  and $L_1$, and `df1` is the number of degrees of freedom of $L_1$. `F01` is a test statistic to decide between the null hypothesis:\n",
    "\n",
    "$$\n",
    "    H_{0} = \\mbox{``The new predictor does not improve the model\"}. \n",
    "$$\n",
    "\n",
    "versus the alternative that it does. When the models are Gauss-Markov normal, `F01` follows a Fisher-Snedecor $F$ distribution with 1 and `df1` degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\operatorname{AIC}$ (Akaike Information Criterion) is an answer to a different problem: adding a predictor to a prediction model means we are adding a parameter to the prediction function (a linear function in this case) and, in general, increasing the number of parameters in a prediction function learned from a given training dataset tends to fit better this particular dataset at the cost of deteriorating the quality of predictions for other samples _from the same statistical population_ from which the training sample was extracted.\n",
    "\n",
    "This is another instance of the bias/variance tradeoff. \n",
    "\n",
    "$\\operatorname{AIC}$ is a _penalized (minus)log-likelihood,_ the difference:\n",
    "\n",
    "$$\n",
    "    -\\log\\cal{L}+\\lambda\\,\\cal{P}, \\mskip30mu \\lambda>0,\n",
    "$$\n",
    "\n",
    "where an increase in log-likelihood gained by adding a new parameter can be cancelled by an increase in the penalization term $\\cal{P}$. In this way we can tell the better from two _nested_ models (a pair of models where the set of predictors in one of them is a subset of that in the other). \n",
    "\n",
    "The better model is the one with a smaller $\\operatorname{AIC}$ (tending to a larger likelihood). In R the $\\operatorname{AIC}$ of a model $L$ can be obtained with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractAIC(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which uses the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC = - 2*log L + k * edf,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, for a linear model, (DAAG pag. 153):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIC= n*log(ResSS/n)+2*Regdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `hills` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loghills.l0<-lm(logtime~logdist,data=loghills)\n",
    "loghills.l1<-lm(logtime~logdist+logclimb,data=loghills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function extracts a list of relevant quantities from an object `L` of the `lm` class (the ones from last session, plus $\\operatorname{AIC}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression.Quantities<-function(L){\n",
    "    y<-L$model[[1]]\n",
    "    y0<-y-mean(y)\n",
    "    TotalSS0<-sum(y0^2)\n",
    "    n<-length(y)\n",
    "    Totaldf<-n\n",
    "    Totaldf0<-Totaldf-1\n",
    "#\n",
    "    yhat<-as.numeric(L$fitted.values)\n",
    "    yhat0<-yhat-mean(yhat)\n",
    "    RegSS0<-sum(yhat0^2)\n",
    "    Regdf<-L$rank\n",
    "    Regdf0<-Regdf-1\n",
    "#\n",
    "    ytilde<-as.numeric(L$residuals)\n",
    "    ResSS<-sum(ytilde^2)\n",
    "    Resdf<-Totaldf0-Regdf0\n",
    "#\n",
    "    TotalMeanS0<-TotalSS0/Totaldf0\n",
    "    RegMeanS0<-RegSS0/Regdf0\n",
    "    ResMeanS<-ResSS/Resdf\n",
    "#\n",
    "    R2<-RegSS0/TotalSS0\n",
    "    RegF<-RegMeanS0/ResMeanS\n",
    "    AIC=n*log(ResSS/n)+2*Regdf\n",
    "#\n",
    "    Q<-list(\n",
    "        y=y,y0=y0,n=n,TotalSS0=TotalSS0,Totaldf=Totaldf,Totaldf0=Totaldf0,\n",
    "        yhat=yhat,yhat0=yhat0,RegSS0=RegSS0,Regdf=Regdf,Regdf0=Regdf0,\n",
    "        ytilde=ytilde,ResSS=ResSS,Resdf=Resdf,\n",
    "        TotalMeanS0=TotalMeanS0,RegMeanS0=RegMeanS0,ResMeanS=ResMeanS,\n",
    "        R2=R2,RegF=RegF,AIC=AIC)\n",
    "    return(Q)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Regression.Quantities()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loghills.Q0<-Regression.Quantities(loghills.l0)\n",
    "loghills.Q1<-Regression.Quantities(loghills.l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, decide which is the better model, according to the above criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Insert your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stats` package has two functions, `add1()` and `drop1`, allowing us to perform this operation in a semi-automatic way. Either we add a predictor to the basic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1(loghills.l0,\"logclimb\")\n",
    "add1(loghills.l0,\"logclimb\",test=\"F\")\n",
    "add1(loghills.l0,\"logclimb\",test=\"Chisq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or drop a variable from a larger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1(loghills.l1,\"logclimb\")\n",
    "drop1(loghills.l1,\"logclimb\",test=\"F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1(loghills.l1,\"logdist\")\n",
    "drop1(loghills.l1,\"logdist\",test=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  `Steam` dataset\n",
    "\n",
    "Files: `Steam.dat.txt` (Data description),  `Steam1.txt` (Actual data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steam<-read.table(\"Steam1.txt\",header=TRUE)\n",
    "str(Steam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "With the `Steam` dataset, find the best subset of predictors of `y` from `(x1,x2,x3,x4,x5,x6,x7,x8,x9)`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such data as `Steam`, with $p$ predictors, there is a large number of possible regressions â€“precisely $2^p$, the number of subsets of the set of all predictors- which, in principle, they can be tested by repeatedly applying the above functions, either starting with the base model with no predictors other than \n",
    "intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls0<-lm(y~1, data=Steam)\n",
    "summary(ls0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and successively add predictors or, on the contrary, starting with the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls1<-lm(y~x1+x2+x3+x4+x5+x6+x7+x8+x9, data=Steam)\n",
    "summary(ls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then successively remove them.\n",
    "\n",
    "For instance, we start with `ls0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add1(ls0,\"x1\",test=\"F\")\n",
    "# ...\n",
    "add1(ls0,\"x9\",test=\"F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `x7` is the better addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep this variable and now we try to add a second one following the same procedure, and successively, until we find a satisfactory subset. \n",
    "\n",
    "This is the _forward_ variables selection method. \n",
    "\n",
    "Symmetrically, the _backward_ method starts with the full model `ls1` and sequentially removes variables.\n",
    "\n",
    "It is worth noting that if, for instance, at a certain stage of a _forward_  procedure the current subset of variables is $A$, we have not tested all subsets of $A$ but only those in the path through which we arrived at $A$. To test them we should apply _backward_  selection from $A$ and so on. \n",
    "\n",
    "The `stats` package has a `step()` function to automate this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(ls0,\"~x1+x2+x3+x4+x5+x6+x7+x8+x9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also from the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step(ls1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonably enough, this combined _forward_ and _backward_ procedure is called _stepwise._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MASS` package has analogous functions, `addterm()`, `dropterm()`, and `stepAIC()`, with the same functionality as the above functions in the basic `stats` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is the strategy of computing all the possible $2^p$ regressions, with all the subsets in the set of predictors, from $\\emptyset$ to the total number $p$ of predictors, and then comparing all the resulting $\\operatorname{AIC}$ values, keeping the best one. \n",
    "\n",
    "This is the _All Subsets Regression_ method. Obviously, this computation is unfeasible by a brute force approach, except for models with a very small number of predictors. There are combinatorial _branch and bound_ algorithms allowing us to limit the number of actual regressions we need to evaluate in full. The method depends on imposing a partial ordering on the set of all possible regressions, giving it a tree-like structure, in which we will need to evaluate only a small number of nodes. The `leaps` package implements this procedure (the name originates from the idiom  _leaps and bounds)._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
